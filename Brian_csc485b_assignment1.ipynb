{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasLund5/CSC485B_A1_IntroToCuda/blob/main/Brian_csc485b_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O cpp_plugin.py https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n",
        "%load_ext cpp_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TsdvUsW_1Cu",
        "outputId": "8afa360d-88b4-48b9-96a0-400b821b6f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-06 01:49:44--  https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n",
            "Resolving gist.github.com (gist.github.com)... 140.82.112.3\n",
            "Connecting to gist.github.com (gist.github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://gist.githubusercontent.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py [following]\n",
            "--2024-10-06 01:49:44--  https://gist.githubusercontent.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2730 (2.7K) [text/plain]\n",
            "Saving to: ‘cpp_plugin.py’\n",
            "\n",
            "cpp_plugin.py       100%[===================>]   2.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-06 01:49:44 (38.7 MB/s) - ‘cpp_plugin.py’ saved [2730/2730]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dclGnLGAgbtH",
        "outputId": "4503c7dd-6b81-4e76-db1f-bb2bf82a356a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-vs3j495g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-vs3j495g\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10743 sha256=3a570d3ab74e9051a615aa78d54722c78a69a0c83a1d407fccde14c37a30e96c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ftg8jia/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp8a_hcov7\".\n"
          ]
        }
      ],
      "source": [
        "# Load the extension that allows us to compile CUDA code in python notebooks\n",
        "# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVbDQthwogQF"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_types.h\"\n",
        "# /**\n",
        "#  * A collection of commonly used data types throughout this project.\n",
        "#  */\n",
        "#pragma once\n",
        "\n",
        "#include <stdint.h> // uint32_t\n",
        "\n",
        "using element_t = uint32_t;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqET4uI2ggwf"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n",
        "/**\n",
        " * Standard macros that can be useful for error checking.\n",
        " * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <cuda.h>\n",
        "\n",
        "#define CUDA_CALL(exp)                                       \\\n",
        "    do {                                                     \\\n",
        "        cudaError res = (exp);                               \\\n",
        "        if(res != cudaSuccess) {                             \\\n",
        "            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n",
        "                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n",
        "           exit(EXIT_FAILURE);                               \\\n",
        "        }                                                    \\\n",
        "    } while(0)\n",
        "\n",
        "#define CHECK_ERROR(msg)                                             \\\n",
        "    do {                                                             \\\n",
        "        cudaError_t err = cudaGetLastError();                        \\\n",
        "        if(cudaSuccess != err) {                                     \\\n",
        "            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n",
        "                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE);                                      \\\n",
        "        }                                                            \\\n",
        "    } while (0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY0L7rKhoVaZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n",
        "/**\n",
        " * Functions for generating random input data with a fixed seed\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <random>  // for std::mt19937, std::uniform_int_distribution\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "\n",
        "/**\n",
        " * Generates and returns a vector of random uniform data of a given length, n,\n",
        " * for any integral type. Input range will be [0, 2n].\n",
        " */\n",
        "template < typename T >\n",
        "std::vector< T > generate_uniform( std::size_t n )\n",
        "{\n",
        "    // for details of random number generation, see:\n",
        "    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n",
        "    std::size_t random_seed = 20240916;  // use magic seed\n",
        "    std::mt19937 rng( random_seed );     // use mersenne twister generator\n",
        "    std::uniform_int_distribution<> distrib(0, 2 * n);\n",
        "\n",
        "    std::vector< T > random_data( n ); // init array\n",
        "    std::generate( std::begin( random_data )\n",
        "                 , std::end  ( random_data )\n",
        "                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n",
        "\n",
        "    return random_data;\n",
        "}\n",
        "\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJOKRZuCkDh2"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n",
        "#pragma once\n",
        "\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "namespace cpu {\n",
        "\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace cpu\n",
        "\n",
        "\n",
        "namespace gpu {\n",
        "\n",
        "void run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3lAuiBEhKjc"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n",
        "/**\n",
        " * CPU methods that the GPU should outperform.\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <algorithm> // std::sort()\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1      {\n",
        "namespace cpu     {\n",
        "\n",
        "/**\n",
        " * Simple solution that just sorts the whole array with a built-in sort\n",
        " * function and then resorts the last portion in the opposing order with\n",
        " * a second call to that same built-in sort function.\n",
        " */\n",
        "void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    std::sort( data, data + num_elements, std::less< element_t >{} );\n",
        "    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n",
        "}\n",
        "\n",
        "/**\n",
        " * Run the single-threaded CPU baseline that students are supposed to outperform\n",
        " * in order to obtain higher grades on this assignment. Times the execution and\n",
        " * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n",
        " * the functions takes the input by value so as to not perturb the original data\n",
        " * in place.\n",
        " */\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n",
        "{\n",
        "    auto const cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort( data.data(), switch_at, n );\n",
        "    auto const cpu_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"CPU Baseline time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace cpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjTbQ3EO2NwQ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n",
        "/**\n",
        " * The file in which you will implement your GPU solutions!\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "namespace gpu {\n",
        "\n",
        "\n",
        "/**\n",
        " * The CPU baseline benefits from warm caches because the data was generated on\n",
        " * the CPU. Run the data through the GPU once with some arbitrary logic to\n",
        " * ensure that the GPU cache is warm too and the comparison is more fair.\n",
        " */\n",
        "__global__\n",
        "void warm_the_gpu(element_t *data, std::size_t invert_at_pos, std::size_t num_elements) {\n",
        "    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (th_id < num_elements && data[th_id] > num_elements * 100) {\n",
        "        ++data[th_id];\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Bitonic Sort for small arrays (n < 1024)\n",
        " */\n",
        "__device__\n",
        "void full_sort(element_t *data, std::size_t num_elements,std::size_t invert_at_pos ) {\n",
        "    // Use shared memory to sort small arrays entirely within a block\n",
        "    __shared__ element_t s_data[1024];\n",
        "\n",
        "    int idx = threadIdx.x;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (idx < num_elements) {\n",
        "        s_data[idx] = data[idx];\n",
        "    } else {\n",
        "        s_data[idx] = INT_MAX;  // Assuming element_t is int\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Bitonic sort in shared memory\n",
        "    for (unsigned int k = 2; k <= num_elements; k <<= 1) {\n",
        "        for (unsigned int j = k >> 1; j > 0; j >>= 1) {\n",
        "            unsigned int ixj = idx ^ j;\n",
        "\n",
        "            if (ixj < num_elements) {\n",
        "                if (ixj > idx) {\n",
        "                    if ((idx & k) == 0) {\n",
        "                        if (s_data[idx] > s_data[ixj]) {\n",
        "                            // Swap\n",
        "                            element_t temp = s_data[idx];\n",
        "                            s_data[idx] = s_data[ixj];\n",
        "                            s_data[ixj] = temp;\n",
        "                        }\n",
        "                    } else {\n",
        "                        if (s_data[idx] < s_data[ixj]) {\n",
        "                            // Swap\n",
        "                            element_t temp = s_data[idx];\n",
        "                            s_data[idx] = s_data[ixj];\n",
        "                            s_data[ixj] = temp;\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Write sorted data back to global memory\n",
        "    // Use the calculations to determine correct position\n",
        "    if (idx >= invert_at_pos && idx < num_elements) {\n",
        "        data[num_elements - idx + invert_at_pos - 1] = s_data[idx];\n",
        "    } else if (idx < invert_at_pos && idx < num_elements) {\n",
        "        data[idx] = s_data[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Bitonic Sort for larger arrays (n >= 1024) using shared memory\n",
        " */\n",
        "__device__\n",
        "void shared_full_sort(element_t *data, std::size_t num_elements,std::size_t invert_at_pos ) {\n",
        "\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int local_idx = threadIdx.x;\n",
        "\n",
        "    // Each block sorts its own subarray\n",
        "    __shared__ element_t s_data[1024];\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (idx < num_elements) {\n",
        "        s_data[local_idx] = data[idx];\n",
        "    } else {\n",
        "        s_data[local_idx] = INT_MAX;  // Assuming element_t is int\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform bitonic sort in shared memory\n",
        "    // blockDim.x should be a power of 2\n",
        "    for (unsigned int k = 2; k <= blockDim.x+1; k <<= 1) {\n",
        "        for (unsigned int j = k >> 1; j > 0; j >>= 1) {\n",
        "            unsigned int ixj = local_idx ^ j;\n",
        "\n",
        "            if (ixj > local_idx) {\n",
        "                if ((local_idx & k) == 0) {\n",
        "                    if (s_data[local_idx] > s_data[ixj]) {\n",
        "                        // Swap\n",
        "                        element_t temp = s_data[local_idx];\n",
        "                        s_data[local_idx] = s_data[ixj];\n",
        "                        s_data[ixj] = temp;\n",
        "                    }\n",
        "                } else {\n",
        "                    if (s_data[local_idx] < s_data[ixj]) {\n",
        "                        // Swap\n",
        "                        element_t temp = s_data[local_idx];\n",
        "                        s_data[local_idx] = s_data[ixj];\n",
        "                        s_data[ixj] = temp;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Write sorted data back to global memory\n",
        "    // Use the calculations to determine correct position\n",
        "    //if (idx >= invert_at_pos && idx < num_elements) {\n",
        "      //  data[num_elements - idx + invert_at_pos - 1] = s_data[local_idx];\n",
        "    //} else if (idx < invert_at_pos && idx < num_elements) {\n",
        "        data[idx] = s_data[local_idx];\n",
        "    //}\n",
        "}\n",
        "\n",
        "__device__\n",
        "void merge_inplace_with_inversion(element_t *data, std::size_t num_elements,std::size_t invert_at_pos ) {\n",
        "    int left = 0;                                      // Pointer to start of the array\n",
        "    int right = num_elements / 2;                      // Pointer to the middle of the array\n",
        "    int end = num_elements - 1;                        // Pointer to end of the array\n",
        "\n",
        "    // Temporary variable for swapping\n",
        "    int temp;\n",
        "\n",
        "    // In-place merge\n",
        "    while (left < right && right < num_elements) {\n",
        "        // Compare and merge normally for elements before 3/4\n",
        "        if (left < invert_at_pos && right < invert_at_pos) {\n",
        "            if (data[left] > data[right]) {\n",
        "                // Swap elements if left is larger than right\n",
        "                temp = data[left];\n",
        "                data[left] = data[right];\n",
        "                data[right] = temp;\n",
        "            }\n",
        "            left++;\n",
        "        }\n",
        "        // For elements past the 3/4 point, invert their order (reverse)\n",
        "        else if (left >= invert_at_pos || right >= invert_at_pos) {\n",
        "            int reverse_right = end - (right - invert_at_pos);  // Calculate inverted position\n",
        "            if (data[left] < data[reverse_right]) {\n",
        "                // Swap elements with their inverted position\n",
        "                temp = data[left];\n",
        "                data[left] = data[reverse_right];\n",
        "                data[reverse_right] = temp;\n",
        "            }\n",
        "            left++;\n",
        "            right++;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Main kernel function\n",
        " */\n",
        "__global__\n",
        "void opposing_sort(element_t *data, std::size_t invert_at_pos, std::size_t num_elements) {\n",
        "    if (num_elements <= 1024) {\n",
        "        full_sort(data, num_elements, invert_at_pos);\n",
        "    } else {\n",
        "        shared_full_sort(data, num_elements, invert_at_pos);\n",
        "        //merge_inplace_with_inversion(data, num_elements, invert_at_pos);\n",
        "    }\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "/**\n",
        " * Performs all the logic of allocating device vectors and copying host/input\n",
        " * vectors to the device. Times the opposing_sort() kernel with wall time,\n",
        " * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n",
        " */\n",
        "void run_gpu_soln(std::vector<element_t> data, std::size_t switch_at, std::size_t n) {\n",
        "    std::size_t const threads_per_block = 1024;\n",
        "    std::size_t const num_blocks = (n + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "    // Allocate arrays on the device/GPU\n",
        "    element_t *d_data;\n",
        "    cudaMalloc((void**)&d_data, sizeof(element_t) * n);\n",
        "    CHECK_ERROR(\"Allocating input array on device\");\n",
        "\n",
        "    // Copy the input from the host to the device/GPU\n",
        "    cudaMemcpy(d_data, data.data(), sizeof(element_t) * n, cudaMemcpyHostToDevice);\n",
        "    CHECK_ERROR(\"Copying input array to device\");\n",
        "\n",
        "    // Warm the cache on the GPU for a more fair comparison\n",
        "    warm_the_gpu<<<num_blocks, threads_per_block>>>(d_data, switch_at, n);\n",
        "\n",
        "    // Time the execution of the kernel that you implemented\n",
        "    auto const kernel_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort<<<num_blocks, threads_per_block>>>(d_data, switch_at, n);\n",
        "    auto const kernel_end = std::chrono::high_resolution_clock::now();\n",
        "    CHECK_ERROR(\"Executing kernel on device\");\n",
        "\n",
        "    // After the timer ends, copy the result back, free the device vector,\n",
        "    // and echo out the timings and the results.\n",
        "    cudaMemcpy(data.data(), d_data, sizeof(element_t) * n, cudaMemcpyDeviceToHost);\n",
        "    CHECK_ERROR(\"Transferring result back to host\");\n",
        "    cudaFree(d_data);\n",
        "    CHECK_ERROR(\"Freeing device memory\");\n",
        "\n",
        "    std::cout << \"GPU Solution time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for (auto const x : data) std::cout << x << \" \";\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-E473UYf_wwg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRvVeK-QifnZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"main.cu\"\n",
        "/**\n",
        " * Driver for the benchmark comparison. Generates random data,\n",
        " * runs the CPU baseline, and then runs your code.\n",
        " */\n",
        "\n",
        "#include <cstddef>  // std::size_t type\n",
        "#include <iostream> // std::cout, std::endl\n",
        "#include <vector>\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "#include \"data_generator.h\"\n",
        "#include \"data_types.h\"\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "int main()\n",
        "{\n",
        "    std::size_t const n =2048;\n",
        "    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n",
        "\n",
        "    auto data = csc485b::a1::generate_uniform< element_t >( n );\n",
        "    csc485b::a1::cpu::run_cpu_baseline( data, switch_at, n );\n",
        "    csc485b::a1::gpu::run_gpu_soln( data, switch_at, n );\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7F0eVsGjUNp",
        "outputId": "67976cce-8847-47ff-9455-c9821bc54abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Baseline time: 118416 ns\n",
            "2 2 4 4 9 10 13 13 15 27 34 36 36 39 41 43 43 45 45 50 51 54 56 58 60 61 61 63 69 71 72 75 78 78 79 79 81 82 83 83 87 92 98 98 103 103 105 105 107 108 108 118 118 124 130 130 132 134 134 135 137 139 139 141 142 143 148 150 150 151 155 160 162 163 164 167 167 167 170 172 181 182 184 188 193 194 197 199 199 202 206 208 209 209 212 215 216 220 221 221 226 229 230 232 234 237 237 238 238 238 241 241 243 244 245 248 255 263 264 267 269 269 278 282 282 282 288 298 300 300 305 310 310 313 316 317 323 328 329 335 336 337 337 340 340 344 346 346 349 350 350 352 353 355 359 362 366 367 369 370 372 380 383 383 386 387 390 392 396 399 399 400 403 403 404 404 405 406 410 410 412 415 417 420 420 420 422 423 424 424 426 428 428 431 432 435 435 436 436 441 442 442 443 444 446 453 453 457 458 459 464 464 465 473 476 477 477 478 480 483 483 484 488 490 493 494 496 498 507 508 508 509 509 511 511 512 516 520 520 521 522 524 532 533 535 540 541 547 548 550 551 552 554 555 558 562 566 567 568 568 569 570 571 572 573 573 573 574 582 588 590 591 593 594 594 598 600 600 601 606 607 608 611 612 613 614 616 620 624 625 625 629 630 631 631 633 639 642 644 646 646 646 649 651 652 652 654 656 659 667 668 670 678 679 682 682 689 690 690 691 696 698 701 707 707 716 720 723 723 725 732 735 736 738 740 743 744 748 752 755 757 757 762 762 768 768 772 772 773 774 779 779 780 782 784 784 787 789 789 791 791 793 795 799 802 804 805 806 810 810 812 817 819 821 825 827 827 828 828 831 834 839 842 846 851 855 857 860 861 870 871 877 877 880 881 881 885 886 886 886 887 888 889 889 892 892 896 898 899 904 904 905 907 908 914 914 915 915 916 920 921 924 929 929 930 932 934 934 934 936 942 943 943 946 947 947 947 953 957 959 960 961 964 965 967 967 967 971 972 973 974 976 977 980 982 989 995 998 1010 1012 1015 1017 1020 1021 1023 1023 1024 1024 1028 1029 1030 1030 1038 1039 1039 1040 1042 1042 1045 1048 1050 1055 1065 1066 1066 1071 1071 1079 1082 1084 1086 1089 1095 1096 1098 1100 1104 1104 1104 1105 1111 1117 1118 1119 1120 1120 1124 1125 1126 1128 1130 1132 1135 1140 1143 1143 1143 1146 1148 1149 1151 1152 1155 1160 1160 1165 1165 1170 1170 1173 1177 1177 1182 1183 1183 1186 1186 1187 1190 1193 1193 1194 1199 1199 1200 1201 1202 1210 1214 1215 1215 1216 1217 1217 1217 1218 1218 1225 1227 1228 1230 1232 1233 1233 1235 1238 1239 1240 1241 1243 1243 1243 1243 1247 1253 1253 1256 1259 1262 1264 1269 1269 1270 1274 1279 1280 1281 1286 1298 1299 1305 1307 1307 1308 1316 1317 1318 1319 1321 1322 1322 1322 1324 1332 1334 1334 1341 1345 1348 1349 1354 1356 1356 1356 1357 1358 1359 1359 1360 1360 1363 1365 1368 1370 1371 1372 1372 1375 1375 1376 1377 1380 1381 1386 1387 1390 1392 1396 1396 1397 1397 1398 1402 1403 1403 1405 1406 1408 1410 1411 1412 1413 1416 1418 1420 1423 1423 1427 1431 1435 1435 1437 1446 1447 1448 1448 1452 1452 1454 1454 1457 1457 1458 1459 1460 1461 1461 1463 1468 1473 1474 1479 1483 1484 1484 1486 1488 1488 1490 1500 1502 1503 1506 1508 1508 1508 1509 1510 1513 1514 1515 1515 1518 1522 1524 1525 1526 1532 1534 1535 1536 1536 1537 1538 1538 1540 1545 1548 1552 1552 1554 1554 1556 1557 1559 1562 1562 1564 1565 1565 1568 1568 1569 1571 1571 1573 1575 1576 1579 1580 1580 1581 1582 1583 1585 1586 1587 1588 1593 1594 1594 1596 1596 1598 1599 1600 1600 1600 1604 1606 1607 1608 1608 1608 1609 1612 1612 1614 1614 1614 1615 1619 1622 1622 1628 1630 1631 1632 1632 1636 1639 1646 1647 1647 1649 1653 1654 1656 1657 1658 1661 1662 1663 1665 1668 1669 1673 1673 1674 1676 1676 1677 1681 1682 1683 1685 1688 1696 1700 1701 1701 1702 1703 1705 1711 1712 1712 1715 1716 1718 1720 1721 1721 1722 1727 1729 1729 1731 1733 1735 1736 1736 1737 1741 1742 1744 1744 1745 1745 1748 1751 1751 1753 1757 1757 1760 1761 1762 1766 1766 1767 1767 1769 1769 1770 1770 1774 1774 1783 1794 1795 1796 1797 1802 1805 1806 1807 1807 1808 1808 1809 1812 1815 1816 1817 1820 1822 1824 1829 1830 1830 1832 1832 1832 1842 1844 1844 1847 1847 1850 1852 1854 1855 1856 1857 1857 1861 1862 1864 1866 1867 1869 1870 1871 1872 1873 1875 1875 1877 1877 1878 1880 1881 1884 1884 1894 1894 1896 1898 1899 1900 1901 1902 1903 1904 1910 1911 1911 1912 1913 1917 1918 1919 1919 1921 1923 1929 1934 1945 1951 1951 1953 1954 1954 1956 1956 1958 1961 1963 1964 1965 1966 1969 1969 1970 1973 1975 1975 1975 1979 1980 1980 1985 1992 1992 1992 1993 1993 1996 1996 1998 1999 2003 2004 2005 2005 2007 2010 2012 2012 2016 2020 2021 2022 2023 2023 2025 2026 2027 2027 2028 2029 2030 2031 2034 2035 2035 2036 2037 2042 2044 2045 2046 2050 2051 2051 2052 2052 2054 2057 2058 2061 2061 2062 2066 2068 2069 2073 2074 2075 2078 2078 2079 2081 2084 2085 2096 2097 2098 2100 2101 2101 2103 2104 2113 2114 2116 2116 2117 2120 2121 2121 2122 2124 2128 2131 2133 2140 2141 2143 2144 2145 2148 2148 2151 2152 2152 2153 2153 2158 2159 2162 2162 2166 2166 2166 2168 2171 2172 2175 2176 2177 2179 2179 2185 2188 2189 2191 2191 2192 2192 2196 2198 2199 2202 2203 2205 2210 2212 2218 2218 2220 2221 2222 2222 2227 2229 2232 2235 2236 2236 2238 2242 2243 2244 2251 2261 2261 2262 2263 2266 2266 2267 2274 2274 2274 2276 2280 2283 2288 2292 2293 2296 2297 2297 2298 2299 2300 2306 2309 2309 2310 2312 2315 2317 2320 2320 2321 2321 2322 2323 2328 2328 2330 2331 2332 2332 2333 2334 2335 2336 2337 2339 2339 2340 2343 2344 2350 2354 2357 2358 2364 2366 2366 2371 2371 2374 2374 2374 2382 2384 2385 2385 2387 2388 2389 2392 2393 2394 2396 2400 2403 2405 2411 2412 2412 2417 2419 2423 2424 2425 2436 2438 2439 2439 2440 2441 2447 2448 2453 2454 2454 2455 2460 2465 2469 2476 2477 2478 2479 2480 2480 2484 2485 2485 2485 2488 2489 2491 2495 2496 2497 2497 2498 2500 2500 2500 2504 2505 2513 2518 2518 2521 2524 2525 2526 2529 2535 2536 2539 2539 2540 2542 2544 2547 2547 2548 2549 2550 2553 2555 2563 2565 2566 2567 2569 2570 2573 2573 2574 2575 2580 2582 2584 2585 2586 2590 2592 2592 2595 2599 2600 2601 2601 2602 2606 2606 2606 2611 2612 2613 2618 2618 2619 2619 2619 2620 2622 2627 2632 2634 2634 2637 2638 2640 2642 2644 2644 2646 2647 2648 2649 2650 2652 2652 2653 2657 2657 2662 2663 2663 2665 2669 2671 2672 2673 2677 2678 2678 2678 2678 2681 2683 2683 2685 2686 2687 2695 2697 2698 2698 2699 2709 2710 2713 2713 2718 2720 2721 2722 2722 2724 2725 2726 2730 2731 2733 2739 2740 2741 2744 2749 2749 2750 2751 2752 2755 2756 2757 2761 2761 2763 2768 2768 2769 2770 2772 2772 2773 2773 2775 2776 2776 2778 2780 2780 2783 2788 2806 2808 2811 2812 2814 2817 2818 2822 2822 2822 2824 2825 2826 2827 2828 2828 2829 2831 2831 2833 2834 2838 2840 2840 2844 2846 2847 2847 2848 2850 2852 2857 2859 2860 2862 2869 2870 2871 2872 2872 2873 2873 2873 2875 2876 2879 2881 2882 2884 2887 2888 2888 2891 2895 2898 2900 2900 2903 2906 2908 2911 2913 2916 2918 2922 2923 2924 2924 2925 2925 2927 2930 2931 2932 2933 2935 2936 2936 2939 2939 2944 2948 2948 2951 2951 2952 2955 2957 2958 2959 2961 2961 2963 2963 2964 2965 2965 2965 2966 2967 2967 2968 2969 2969 2970 2977 2981 2981 2982 2984 2985 2986 2986 2987 2988 2990 2994 2997 2998 2998 3001 3002 3002 3003 3004 3009 3010 3010 3011 3012 3012 3013 3014 3014 3015 3017 3018 3020 3023 3025 3026 3028 3032 3032 3034 3034 3035 3036 3036 3036 3038 3039 3040 3042 3045 3050 3056 3057 4096 4095 4090 4090 4089 4087 4086 4085 4085 4080 4074 4071 4068 4061 4058 4057 4053 4052 4050 4049 4040 4036 4033 4032 4031 4029 4029 4028 4026 4025 4023 4020 4016 4015 4015 4012 4011 4006 4000 3996 3996 3994 3992 3991 3988 3987 3983 3983 3980 3980 3977 3977 3973 3972 3970 3970 3969 3966 3960 3959 3959 3954 3951 3947 3945 3940 3935 3935 3931 3926 3923 3922 3921 3921 3918 3913 3912 3912 3910 3909 3907 3907 3906 3903 3900 3897 3895 3893 3892 3892 3892 3892 3886 3880 3880 3879 3878 3877 3874 3874 3874 3871 3868 3866 3859 3859 3857 3855 3853 3851 3848 3847 3846 3845 3842 3841 3839 3831 3831 3831 3830 3829 3827 3822 3817 3813 3808 3808 3806 3805 3802 3799 3798 3798 3793 3791 3787 3786 3780 3776 3775 3772 3771 3767 3766 3761 3759 3755 3751 3751 3739 3733 3728 3727 3727 3725 3724 3722 3719 3718 3717 3714 3714 3711 3711 3710 3710 3708 3707 3703 3699 3699 3699 3698 3695 3691 3690 3690 3688 3687 3686 3686 3686 3682 3682 3678 3672 3672 3668 3666 3665 3665 3661 3661 3655 3654 3654 3653 3653 3652 3648 3647 3645 3644 3643 3643 3643 3638 3638 3637 3635 3634 3634 3631 3631 3629 3628 3626 3623 3616 3614 3614 3612 3612 3608 3607 3604 3602 3598 3595 3595 3586 3583 3573 3572 3564 3564 3563 3561 3560 3557 3556 3554 3551 3550 3546 3545 3544 3538 3533 3532 3529 3529 3527 3526 3521 3518 3518 3518 3517 3517 3508 3506 3505 3503 3501 3498 3498 3498 3496 3492 3489 3489 3486 3484 3484 3481 3481 3480 3478 3478 3476 3475 3472 3470 3467 3465 3464 3462 3459 3459 3458 3457 3456 3454 3453 3451 3450 3449 3444 3441 3438 3432 3431 3429 3427 3424 3424 3419 3418 3417 3415 3415 3411 3410 3409 3407 3406 3401 3399 3391 3391 3390 3390 3388 3385 3381 3379 3379 3379 3377 3370 3369 3369 3368 3368 3368 3367 3364 3364 3361 3360 3359 3358 3356 3356 3350 3347 3346 3343 3342 3339 3338 3337 3335 3332 3331 3328 3327 3326 3325 3324 3324 3323 3320 3320 3317 3317 3316 3313 3313 3311 3311 3310 3309 3309 3309 3307 3305 3304 3300 3299 3297 3296 3293 3293 3291 3290 3289 3287 3283 3280 3279 3276 3275 3274 3272 3272 3270 3270 3270 3269 3264 3263 3261 3261 3259 3258 3255 3254 3251 3245 3244 3242 3242 3241 3238 3235 3232 3232 3231 3231 3227 3226 3226 3222 3222 3220 3220 3218 3217 3217 3216 3214 3214 3212 3211 3211 3208 3207 3202 3201 3200 3199 3196 3194 3194 3191 3190 3189 3188 3186 3184 3184 3178 3177 3176 3175 3173 3172 3171 3170 3168 3168 3168 3163 3163 3162 3159 3159 3157 3155 3153 3148 3145 3143 3138 3135 3129 3129 3123 3119 3117 3114 3113 3111 3109 3106 3103 3102 3101 3100 3098 3095 3092 3090 3083 3083 3081 3079 3078 3076 3075 3073 3071 3070 3069 3069 3066 3061 3061 3058 \n",
            "GPU Solution time: 15272 ns\n",
            "4 10 13 15 27 39 41 43 43 45 51 61 71 79 81 82 83 83 92 98 98 103 105 105 107 108 108 118 118 130 134 135 137 142 143 148 150 151 160 162 167 167 172 181 199 202 208 209 209 221 221 226 232 234 237 238 241 245 248 263 267 269 282 282 282 288 300 300 305 310 313 329 335 340 344 346 350 353 355 359 362 369 370 372 386 396 399 399 400 403 404 410 410 412 417 420 424 424 426 428 435 435 436 442 443 446 453 457 464 473 477 477 478 483 488 490 493 494 496 508 508 509 512 516 520 520 521 522 532 535 547 548 551 552 558 567 568 569 572 573 574 582 591 594 598 600 606 607 611 612 620 625 630 631 633 642 644 651 652 652 659 667 668 670 678 679 682 689 690 690 698 701 707 716 723 743 748 762 762 768 772 772 773 774 779 782 787 789 789 791 795 802 806 817 821 827 834 839 842 851 855 857 860 861 870 877 880 881 885 886 886 886 888 899 905 907 914 915 920 929 930 934 934 936 942 943 943 947 959 960 961 967 971 972 973 977 995 998 1012 1017 1021 1023 1023 1024 1028 1042 1045 1048 1050 1066 1071 1082 1084 1089 1095 1098 1100 1104 1104 1105 1111 1117 1118 1119 1120 1120 1126 1130 1143 1146 1148 1149 1151 1152 1160 1160 1170 1182 1183 1183 1187 1190 1193 1199 1202 1210 1215 1216 1217 1217 1218 1218 1225 1227 1228 1230 1233 1235 1241 1243 1243 1262 1264 1279 1286 1307 1308 1317 1319 1322 1322 1324 1332 1334 1334 1348 1354 1356 1357 1359 1359 1360 1365 1372 1375 1377 1386 1390 1392 1396 1396 1397 1397 1402 1403 1405 1410 1411 1413 1418 1423 1427 1431 1435 1437 1446 1452 1457 1458 1459 1460 1463 1474 1484 1486 1488 1500 1503 1509 1513 1514 1515 1515 1518 1522 1526 1532 1534 1536 1556 1559 1568 1571 1575 1580 1580 1581 1582 1583 1586 1588 1598 1600 1608 1612 1612 1619 1622 1628 1630 1631 1632 1636 1646 1647 1654 1658 1661 1668 1673 1673 1688 1702 1703 1705 1711 1712 1712 1715 1718 1721 1722 1727 1729 1733 1735 1736 1742 1744 1751 1757 1760 1761 1762 1766 1767 1767 1769 1794 1796 1797 1802 1806 1807 1808 1809 1815 1816 1817 1820 1824 1830 1830 1842 1844 1847 1847 1850 1852 1856 1864 1870 1871 1875 1877 1877 1878 1880 1884 1884 1894 1902 1904 1910 1911 1911 1918 1929 1945 1951 1954 1956 1961 1963 1969 1970 1975 1975 1980 1985 1992 1993 1993 1996 1998 2004 2007 2010 2020 2022 2023 2023 2025 2026 2027 2030 2035 2037 2045 2050 2051 2051 2052 2052 2054 2057 2062 2068 2073 2074 2075 2078 2078 2096 2097 2098 2101 2101 2103 2113 2114 2116 2121 2122 2128 2131 2141 2144 2145 2152 2158 2162 2162 2166 2166 2168 2171 2172 2176 2179 2188 2191 2198 2199 2203 2210 2218 2221 2229 2232 2236 2238 2244 2251 2261 2261 2263 2267 2274 2274 2283 2292 2293 2297 2309 2309 2312 2315 2320 2320 2321 2334 2335 2337 2339 2343 2344 2357 2358 2364 2366 2366 2374 2374 2374 2382 2384 2385 2385 2387 2388 2393 2394 2396 2400 2412 2419 2424 2425 2439 2440 2441 2453 2454 2455 2479 2480 2485 2489 2491 2495 2496 2497 2497 2498 2500 2500 2513 2518 2518 2521 2524 2525 2536 2539 2544 2547 2550 2555 2563 2565 2567 2569 2574 2575 2580 2586 2590 2592 2595 2599 2601 2601 2606 2619 2620 2622 2634 2638 2640 2644 2646 2647 2648 2649 2650 2657 2657 2662 2663 2663 2665 2669 2673 2678 2678 2683 2685 2686 2687 2698 2698 2699 2709 2713 2718 2722 2724 2733 2739 2741 2749 2751 2755 2756 2761 2761 2768 2769 2772 2772 2773 2775 2778 2783 2788 2806 2812 2817 2818 2826 2828 2831 2831 2834 2847 2848 2852 2857 2859 2860 2862 2869 2870 2871 2872 2875 2876 2879 2881 2882 2888 2895 2898 2903 2906 2908 2911 2918 2922 2924 2930 2933 2935 2939 2948 2951 2951 2958 2959 2961 2965 2966 2967 2968 2977 2981 2982 2986 2986 2987 2988 2994 2998 2998 3001 3002 3002 3003 3010 3010 3012 3013 3023 3025 3026 3032 3036 3036 3038 3040 3045 3057 3061 3066 3069 3070 3071 3076 3078 3079 3083 3083 3095 3101 3102 3103 3106 3114 3117 3129 3143 3145 3153 3155 3159 3168 3168 3170 3171 3172 3176 3178 3191 3199 3200 3202 3208 3211 3212 3214 3216 3217 3222 3226 3227 3231 3231 3232 3232 3241 3242 3244 3254 3259 3261 3261 3263 3269 3270 3270 3279 3290 3291 3293 3293 3296 3300 3304 3305 3307 3309 3309 3310 3311 3313 3313 3317 3317 3320 3327 3331 3337 3339 3342 3343 3356 3359 3361 3364 3368 3369 3379 3385 3388 3390 3391 3391 3401 3411 3415 3415 3418 3419 3441 3444 3449 3450 3453 3454 3462 3476 3478 3478 3481 3481 3484 3486 3489 3489 3492 3498 3498 3501 3508 3518 3518 3521 3527 3529 3532 3533 3538 3544 3545 3546 3551 3554 3556 3557 3560 3563 3564 3564 3583 3586 3595 3595 3612 3612 3616 3623 3629 3631 3631 3634 3634 3637 3638 3638 3643 3644 3645 3652 3654 3654 3655 3661 3665 3665 3672 3672 3690 3698 3699 3703 3707 3710 3710 3714 3718 3722 3724 3727 3727 3739 3751 3761 3766 3775 3776 3780 3786 3806 3808 3817 3822 3830 3839 3846 3848 3851 3853 3857 3868 3874 3874 3878 3892 3895 3897 3900 3906 3907 3909 3910 3912 3913 3918 3921 3931 3935 3947 3959 3969 3970 3972 3977 3977 3980 3983 3987 3996 3996 4000 4006 4011 4012 4015 4015 4023 4032 4036 4052 4053 4058 4071 4074 4080 4085 4086 4089 4090 4095 2 2 4 9 13 34 36 36 45 50 54 56 58 60 61 63 69 72 75 78 78 79 87 103 124 130 132 134 139 139 141 150 155 163 164 167 170 182 184 188 193 194 197 199 206 212 215 216 220 229 230 237 238 238 241 243 244 255 264 269 278 298 310 316 317 323 328 336 337 337 340 346 349 350 352 366 367 380 383 383 387 390 392 403 404 405 406 415 420 420 422 423 428 431 432 436 441 442 444 453 458 459 464 465 476 480 483 484 498 507 509 511 511 524 533 540 541 550 554 555 562 566 568 570 571 573 573 588 590 593 594 600 601 608 613 614 616 624 625 629 631 639 646 646 646 649 654 656 682 691 696 707 720 723 725 732 735 736 738 740 744 752 755 757 757 768 779 780 784 784 791 793 799 804 805 810 810 812 819 825 827 828 828 831 846 871 877 881 887 889 889 892 892 896 898 904 904 908 914 915 916 921 924 929 932 934 946 947 947 953 957 964 965 967 967 974 976 980 982 989 1010 1015 1020 1024 1029 1030 1030 1038 1039 1039 1040 1042 1055 1065 1066 1071 1079 1086 1096 1104 1124 1125 1128 1132 1135 1140 1143 1143 1155 1165 1165 1170 1173 1177 1177 1186 1186 1193 1194 1199 1200 1201 1214 1215 1217 1232 1233 1238 1239 1240 1243 1243 1247 1253 1253 1256 1259 1269 1269 1270 1274 1280 1281 1298 1299 1305 1307 1316 1318 1321 1322 1341 1345 1349 1356 1356 1358 1360 1363 1368 1370 1371 1372 1375 1376 1380 1381 1387 1398 1403 1406 1408 1412 1416 1420 1423 1435 1447 1448 1448 1452 1454 1454 1457 1461 1461 1468 1473 1479 1483 1484 1488 1490 1502 1506 1508 1508 1508 1510 1524 1525 1535 1536 1537 1538 1538 1540 1545 1548 1552 1552 1554 1554 1557 1562 1562 1564 1565 1565 1568 1569 1571 1573 1576 1579 1585 1587 1593 1594 1594 1596 1596 1599 1600 1600 1604 1606 1607 1608 1608 1609 1614 1614 1614 1615 1622 1632 1639 1647 1649 1653 1656 1657 1662 1663 1665 1669 1674 1676 1676 1677 1681 1682 1683 1685 1696 1700 1701 1701 1716 1720 1721 1729 1731 1736 1737 1741 1744 1745 1745 1748 1751 1753 1757 1766 1769 1770 1770 1774 1774 1783 1795 1805 1807 1808 1812 1822 1829 1832 1832 1832 1844 1854 1855 1857 1857 1861 1862 1866 1867 1869 1872 1873 1875 1881 1894 1896 1898 1899 1900 1901 1903 1912 1913 1917 1919 1919 1921 1923 1934 1951 1953 1954 1956 1958 1964 1965 1966 1969 1973 1975 1979 1980 1992 1992 1996 1999 2003 2005 2005 2012 2012 2016 2021 2027 2028 2029 2031 2034 2035 2036 2042 2044 2046 2058 2061 2061 2066 2069 2079 2081 2084 2085 2100 2104 2116 2117 2120 2121 2124 2133 2140 2143 2148 2148 2151 2152 2153 2153 2159 2166 2175 2177 2179 2185 2189 2191 2192 2192 2196 2202 2205 2212 2218 2220 2222 2222 2227 2235 2236 2242 2243 2262 2266 2266 2274 2276 2280 2288 2296 2297 2298 2299 2300 2306 2310 2317 2321 2322 2323 2328 2328 2330 2331 2332 2332 2333 2336 2339 2340 2350 2354 2371 2371 2389 2392 2403 2405 2411 2412 2417 2423 2436 2438 2439 2447 2448 2454 2460 2465 2469 2476 2477 2478 2480 2484 2485 2485 2488 2500 2504 2505 2526 2529 2535 2539 2540 2542 2547 2548 2549 2553 2566 2570 2573 2573 2582 2584 2585 2592 2600 2602 2606 2606 2611 2612 2613 2618 2618 2619 2619 2627 2632 2634 2637 2642 2644 2652 2652 2653 2671 2672 2677 2678 2678 2681 2683 2695 2697 2710 2713 2720 2721 2722 2725 2726 2730 2731 2740 2744 2749 2750 2752 2757 2763 2768 2770 2773 2776 2776 2780 2780 2808 2811 2814 2822 2822 2822 2824 2825 2827 2828 2829 2833 2838 2840 2840 2844 2846 2847 2850 2872 2873 2873 2873 2884 2887 2888 2891 2900 2900 2913 2916 2923 2924 2925 2925 2927 2931 2932 2936 2936 2939 2944 2948 2952 2955 2957 2961 2963 2963 2964 2965 2965 2967 2969 2969 2970 2981 2984 2985 2990 2997 3004 3009 3011 3012 3014 3014 3015 3017 3018 3020 3028 3032 3034 3034 3035 3036 3039 3042 3050 3056 3058 3061 3069 3073 3075 3081 3090 3092 3098 3100 3109 3111 3113 3119 3123 3129 3135 3138 3148 3157 3159 3162 3163 3163 3168 3173 3175 3177 3184 3184 3186 3188 3189 3190 3194 3194 3196 3201 3207 3211 3214 3217 3218 3220 3220 3222 3226 3235 3238 3242 3245 3251 3255 3258 3264 3270 3272 3272 3274 3275 3276 3280 3283 3287 3289 3297 3299 3309 3311 3316 3320 3323 3324 3324 3325 3326 3328 3332 3335 3338 3346 3347 3350 3356 3358 3360 3364 3367 3368 3368 3369 3370 3377 3379 3379 3381 3390 3399 3406 3407 3409 3410 3417 3424 3424 3427 3429 3431 3432 3438 3451 3456 3457 3458 3459 3459 3464 3465 3467 3470 3472 3475 3480 3484 3496 3498 3503 3505 3506 3517 3517 3518 3526 3529 3550 3561 3572 3573 3598 3602 3604 3607 3608 3614 3614 3626 3628 3635 3643 3643 3647 3648 3653 3653 3661 3666 3668 3678 3682 3682 3686 3686 3686 3687 3688 3690 3691 3695 3699 3699 3708 3711 3711 3714 3717 3719 3725 3728 3733 3751 3755 3759 3767 3771 3772 3787 3791 3793 3798 3798 3799 3802 3805 3808 3813 3827 3829 3831 3831 3831 3841 3842 3845 3847 3855 3859 3859 3866 3871 3874 3877 3879 3880 3880 3886 3892 3892 3892 3893 3903 3907 3912 3921 3922 3923 3926 3935 3940 3945 3951 3954 3959 3960 3966 3970 3973 3980 3983 3988 3991 3992 3994 4016 4020 4025 4026 4028 4029 4029 4031 4033 4040 4049 4050 4057 4061 4068 4085 4087 4090 4096 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}